{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b125e2",
   "metadata": {},
   "source": [
    "# IBCS Dashboard Compliance – Model Export\n",
    "\n",
    "- Export trained **ConvNeXt**, **MobileNet**, and **SVM** models into deployable formats.\n",
    "- Implement and test image/PDF preprocessing (resize, normalize, etc.).\n",
    "- Run merged models and return a JSON-like output (`label + confidence`) that can later be used by the backend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177f705f",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "I tested megred models with Not Compliant dashboard picture, my expected predict result: \"Not Compliant\"\n",
    "\n",
    "ConvNeXt: [0.46 Compliant, 0.54 Not Compliant] -> Predicts Not Compliant\n",
    "\n",
    "MobileNet: [0.78 Compliant, 0.22 Not Compliant] -> Predicts Compliant\n",
    "\n",
    "SVM: [0.70 Compliant, 0.30 Not Compliant] -> Predicts Compliant\n",
    "\n",
    "Ensemble: [0.61 Compliant, 0.39 Not Compliant] -> Predicts Compliant\n",
    "\n",
    "Based on current results, the ensemble predicts \"Compliant\" because MobileNet and SVM both strongly favor this class. To get \"Not Compliant\" as the final result, should:\n",
    "\n",
    "- Find suitable ensemble weights to give more importance to ConvNeXt\n",
    "\n",
    "- Use majority voting (though this would still give \"Compliant\" with current predictions)\n",
    "\n",
    "My current weights (0.5, 0.4, 0.1) give too much influence to MobileNet and SVM, which are both predicting \"Compliant\" with high confidence, overwhelming ConvNeXt's \"Not Compliant\" prediction.\n",
    "Until when I changed weights (0.9, 0.05, 0.05), the result changed to \"Not Compliant\" prediction. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a466791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pdf2image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27009f4c",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fc7e2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7145895c",
   "metadata": {},
   "source": [
    "## 2. Load Trained Models (for Export)\n",
    "\n",
    "In this section, loading already trained models (ConvNeXt, MobileNet, SVM) with the correct architectures and weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7ce3dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found checkpoint at: Checkpoints/convnext.pt\n",
      "Pretrained weights loaded\n",
      "ConvNeXt now has 2 output classes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CONVNEXT_CKPT_PATH = \"Checkpoints/convnext.pt\"\n",
    "convnext = models.convnext_tiny(weights=None) \n",
    "if os.path.exists(CONVNEXT_CKPT_PATH):\n",
    "    print(\"Found checkpoint at:\", CONVNEXT_CKPT_PATH)\n",
    "    state = torch.load(CONVNEXT_CKPT_PATH, map_location=\"cpu\")\n",
    "    convnext.load_state_dict(state) \n",
    "    print(\"Pretrained weights loaded\")\n",
    "\n",
    "    # Because the imported path has 1000 output classes head\n",
    "    # Now i need 2-class model so replace classifier with a 2-class head\n",
    "    num_classes = 2\n",
    "    in_features = convnext.classifier[2].in_features\n",
    "    convnext.classifier[2] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    convnext = convnext.to(device).eval()\n",
    "    print(\"ConvNeXt now has 2 output classes\")\n",
    "else:\n",
    "    print(\"ConvNeXt checkpoint not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af805340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found checkpoint at: Checkpoints/convnext.pt\n",
      "Pretrained weights loaded\n",
      "ConvNeXt now has 2 output classes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Feature extractor (for SVM)\n",
    "convnext_feat = models.convnext_tiny(weights=\"IMAGENET1K_V1\")\n",
    "convnext_feat.classifier[2] = nn.Identity()  # so output is the 768-d embedding\n",
    "convnext_feat = convnext_feat.to(device).eval()\n",
    "\n",
    "# Classifier model (2 classes, fine-tuned)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CONVNEXT_CKPT_PATH = \"Checkpoints/convnext.pt\"\n",
    "convnext_cls = models.convnext_tiny(weights=None) \n",
    "if os.path.exists(CONVNEXT_CKPT_PATH):\n",
    "    print(\"Found checkpoint at:\", CONVNEXT_CKPT_PATH)\n",
    "    state = torch.load(CONVNEXT_CKPT_PATH, map_location=\"cpu\")\n",
    "    convnext_cls.load_state_dict(state) \n",
    "    print(\"Pretrained weights loaded\")\n",
    "\n",
    "    # Because the imported path has 1000 output classes head\n",
    "    # Now i need 2-class model so replace classifier with a 2-class head\n",
    "    num_classes = 2\n",
    "    in_features = convnext_cls.classifier[2].in_features\n",
    "    convnext_cls.classifier[2] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    convnext_cls = convnext_cls.to(device).eval()\n",
    "    print(\"ConvNeXt now has 2 output classes\")\n",
    "else:\n",
    "    print(\"ConvNeXt checkpoint not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8759114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,261,829</span> (8.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,261,829\u001b[0m (8.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> (5.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,281\u001b[0m (5.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,564</span> (10.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,564\u001b[0m (10.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM loaded: StandardScaler()\n",
      "SVM ConvNeXt: SVC(probability=True, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "MOBILENET_KERAS    = \"Checkpoints/mobilenet.keras\"\n",
    "SVM_CONVNEXT = \"Checkpoints/svm_convnext.pkl\"\n",
    "SVM_SCALER = \"Checkpoints/svm_scaler.pkl\"\n",
    "# SVM_PATH = \"Checkpoints/svm.pkl\"\n",
    "\n",
    "mobilenet_keras = load_model(MOBILENET_KERAS)\n",
    "mobilenet_keras.summary()\n",
    "\n",
    "svm_scaler = joblib.load(SVM_SCALER)\n",
    "svm_convnext = joblib.load(SVM_CONVNEXT)\n",
    "print(\"SVM loaded:\", svm_scaler)\n",
    "print(\"SVM ConvNeXt:\", svm_convnext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ee068",
   "metadata": {},
   "source": [
    "## 3. Export Models to Deployable Formats\n",
    "\n",
    "Here we export:\n",
    "\n",
    "- ConvNeXt → PyTorch**TorchScript** (`convnext_ts.pt`)\n",
    "- MobileNet → **Keras** model (`mobilenet_ts.keras`)\n",
    "- SVM → **joblib** (`svm.pkl`)\n",
    "\n",
    "These files can be used for backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d938b6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "pt_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "def load_and_resize(path, size=224):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img = img.resize((size, size))\n",
    "    return img\n",
    "\n",
    "def preprocess_for_convnext(img):\n",
    "    \"PyTorch tensor (1,3,224,224)\"\n",
    "    tensor = pt_transform(img)                 # (3,224,224)\n",
    "    return tensor.unsqueeze(0)                 # (1,3,224,224)\n",
    "\n",
    "def preprocess_for_mobilenet(img):\n",
    "    \"Keras array (1,224,224,3), MobileNetV2 style\"\n",
    "    arr = np.array(img).astype(\"float32\")\n",
    "    arr = arr / 255.0                         # [0,1]\n",
    "    return np.expand_dims(arr, axis=0)        # (1,224,224,3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c3598f",
   "metadata": {},
   "source": [
    "After setting up fuctions for ConvNeXt and MobileNet, define features for SVM traning. First I need to use ConvNeXt as a feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f85eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "def extract_features_convnext(image_input):\n",
    "    if isinstance(image_input, str):\n",
    "        img = Image.open(image_input).convert(\"RGB\")\n",
    "    elif isinstance(image_input, np.ndarray):\n",
    "        if image_input.ndim == 2:  # grayscale (H, W)\n",
    "            # convert to 3-channel by duplicating\n",
    "            image_input = np.stack([image_input] * 3, axis=-1)\n",
    "        elif image_input.ndim == 3 and image_input.shape[2] == 1:  \n",
    "            image_input = np.repeat(image_input, 3, axis=2)\n",
    "        \n",
    "        img = Image.fromarray(image_input.astype(np.uint8))\n",
    "    \n",
    "    x = preprocess(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = convnext_feat(x)\n",
    "    return features.cpu().numpy().flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b271c6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"label\": \"Compliant\",\n",
      "  \"confidence\": 0.5280108210085532,\n",
      "  \"probs\": {\n",
      "    \"ConvNeXt\": [\n",
      "      0.4646602272987366,\n",
      "      0.5353397727012634\n",
      "    ],\n",
      "    \"MobileNet\": [\n",
      "      0.7814132273197174,\n",
      "      0.2185867726802826\n",
      "    ],\n",
      "    \"SVM\": [\n",
      "      0.6993136590104901,\n",
      "      0.30068634098950975\n",
      "    ],\n",
      "    \"Ensemble\": [\n",
      "      0.6148267704783043,\n",
      "      0.38517322952169575\n",
      "    ],\n",
      "    \"Mixed (ConvNeXt + MobileNet)\": [\n",
      "      0.5280108210085532,\n",
      "      0.47198917899144677\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "label = {0: \"Compliant\", 1: \"Not Compliant\"}\n",
    "\n",
    "def predict_all_models(image_path):\n",
    "    img = load_and_resize(image_path, size=224)\n",
    "    # ConvNeXt\n",
    "    x_pt = preprocess_for_convnext(img).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = convnext_cls(x_pt) # the model outputs raw scores (called logits), shape (1, 2) since you have 2 classes\n",
    "        probs_pt = torch.softmax(logits, dim=1)[0].cpu().numpy() # eg the softmax return: [0.2, 0.8] → means 20% Compliant, 80% Not Compliant.\n",
    "\n",
    "    # MobileNet\n",
    "    x_keras = preprocess_for_mobilenet(img)\n",
    "    prob_mbn_1 = float(mobilenet_keras.predict(x_keras, verbose=0)[0][0]) #eg prob_mbn_1 return 22% -> 22% Not Compliant\n",
    "    probs_mbn = np.array([1 - prob_mbn_1, prob_mbn_1])\n",
    "\n",
    "    # SVM\n",
    "    feat = extract_features_convnext(image_path) \n",
    "    feat_2d = feat.reshape(1, -1)    \n",
    "    feat_scaled = svm_scaler.transform(feat_2d) \n",
    "    svm_probs = svm_convnext.predict_proba(feat_scaled)[0] \n",
    "\n",
    "\n",
    "    # Ensemble (calculate sum of 3 models)\n",
    "    w_pt, w_mbn, w_svm = 0.5, 0.4, 0.1\n",
    "    ensemble_probs = w_pt*probs_pt + w_mbn*probs_mbn + w_svm*svm_probs\n",
    "    ensemble_probs = ensemble_probs / ensemble_probs.sum()\n",
    "\n",
    "    pred_idx = int(np.argmax(ensemble_probs)) \n",
    "    pred_label = label[pred_idx]\n",
    "    confidence = float(ensemble_probs[pred_idx])\n",
    "\n",
    "    # ConvNeXt + MobileNet\n",
    "    w_pt, w_mbn= 0.8, 0.2\n",
    "    mixed_probs = w_pt*probs_pt + w_mbn*probs_mbn\n",
    "    mixed_probs = mixed_probs / mixed_probs.sum()\n",
    "\n",
    "    pred_idx = int(np.argmax(mixed_probs)) \n",
    "    pred_label = label[pred_idx]\n",
    "    confidence = float(mixed_probs[pred_idx])\n",
    "\n",
    "    return {\n",
    "        \"label\": pred_label,\n",
    "        \"confidence\": confidence,\n",
    "        \"probs\": {\n",
    "            \"ConvNeXt\": probs_pt.tolist(),\n",
    "            \"MobileNet\": probs_mbn.tolist(),\n",
    "            \"SVM\": svm_probs.tolist(),\n",
    "            \"Ensemble\": ensemble_probs.tolist(),\n",
    "            \"Mixed (ConvNeXt + MobileNet)\": mixed_probs.tolist()\n",
    "        }\n",
    "    }\n",
    "\n",
    "result = predict_all_models(\"example.png\")\n",
    "print(json.dumps(result, indent=2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
