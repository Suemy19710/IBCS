{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddbbca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, Any, Optional, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c16266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 6  # 0: compliant, 1–5: non-compliant by rule\n",
    "CLASS_TO_RULE = {\n",
    "    0: (\"compliant\", None),\n",
    "    1: (\"non_compliant\", \"scaling\"),\n",
    "    2: (\"non_compliant\", \"titles\"),\n",
    "    3: (\"non_compliant\", \"color_misuse\"),\n",
    "    4: (\"non_compliant\", \"axis_check\"),\n",
    "    5: (\"non_compliant\", \"clutter_detection\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8850a4",
   "metadata": {},
   "source": [
    "## MobileNet\n",
    "Create a MobileNetV3-Small model with a custom classifier for 'num_classes' outputs.\n",
    "\n",
    "Create model and optionally load fine-tuned weights (.pth).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "113c8cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mobilenet_rule_model(num_classes: int = NUM_CLASSES) -> nn.Module:\n",
    "    model = models.mobilenet_v3_small(pretrained=True)\n",
    "    # Default classifier is: [Linear, Hardswish, Dropout, Linear]\n",
    "    in_features = model.classifier[3].in_features\n",
    "    model.classifier[3] = nn.Linear(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def load_mobilenet_model(checkpoint_path: Optional[str] = None) -> nn.Module:\n",
    "    model = create_mobilenet_rule_model()\n",
    "    if checkpoint_path is not None and os.path.isfile(checkpoint_path):\n",
    "        state = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(state)\n",
    "        print(f\"Loaded model weights from: {checkpoint_path}\")\n",
    "    else:\n",
    "        if checkpoint_path is not None:\n",
    "            print(f\"Checkpoint not found at {checkpoint_path}, using ImageNet weights only.\")\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c420750",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3c13e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "        std=[0.229, 0.224, 0.225],   # ImageNet std\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(image_path: str) -> torch.Tensor:\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    tensor = preprocess(img)\n",
    "    return tensor  # shape: (3, H, W)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97c5dc4",
   "metadata": {},
   "source": [
    "## Prediction & Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0efb34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dashboard(\n",
    "    model: nn.Module,\n",
    "    image_tensor: torch.Tensor\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run inference on a preprocessed image tensor.\n",
    "    Returns a dict with class_id, label, rule, confidence.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    image_tensor = image_tensor.to(DEVICE).unsqueeze(0)  # (1, 3, H, W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(image_tensor)               # (1, NUM_CLASSES)\n",
    "        probs = F.softmax(logits, dim=1)[0]       # (NUM_CLASSES,)\n",
    "        pred_idx = int(torch.argmax(probs).item())\n",
    "        confidence = float(probs[pred_idx].item())\n",
    "\n",
    "    label, rule = CLASS_TO_RULE[pred_idx]\n",
    "\n",
    "    return {\n",
    "        \"class_id\": pred_idx,\n",
    "        \"label\": label,\n",
    "        \"rule\": rule,\n",
    "        \"confidence\": confidence\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e156873e",
   "metadata": {},
   "source": [
    "## Rule-based Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b26f3e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_feedback(rule: str, label: str, confidence: float, details: dict = None) -> dict:\n",
    "    feedback = {\n",
    "        \"label\": label,\n",
    "        \"confidence\": round(confidence, 2),\n",
    "        \"feedback\": []\n",
    "    }\n",
    "    \n",
    "    # If compliant\n",
    "    if label == 'Compliant':\n",
    "        feedback[\"feedback\"].append(\n",
    "            f\"Great! It follows the {rule if rule else 'IBCS'} rules. This dashboard meets the required compliance standards. Layout, readability, and structure appear suitable.\"\n",
    "            f\"Confidence: {confidence:.0%}.\"\n",
    "        )\n",
    "        return feedback\n",
    "    \n",
    "    # Non-compliant: Give specific, helpful suggestions\n",
    "    suggestions: List[str] = []\n",
    "    \n",
    "    if rule == 'Scaling':\n",
    "        suggestions.extend([\n",
    "            \"**Start bar charts at zero.** If your y-axis starts above zero, it can make small differences look huge.\",\n",
    "            \"**Use the same scale for similar charts.** If you're comparing sales across regions, all charts should have identical y-axis ranges.\",\n",
    "            \"**Show scale breaks clearly.** If you must skip part of the scale, use a visible break symbol (like ~) so readers know.\",\n",
    "            \"**Label your units.** Add '(in EUR)', '(%)' or similar to your axis labels.\",\n",
    "            \"**Avoid dual axes unless absolutely necessary.** Two different scales on one chart confuse readers.\"\n",
    "        ])\n",
    "        \n",
    "        if details:\n",
    "            violations = details.get('violations', [])\n",
    "            if 'axis_misaligned' in violations:\n",
    "                suggestions.insert(0, \"**Axis alignment issue detected.** Check the areas highlighted in red.\")\n",
    "            if 'non_zero_start' in violations:\n",
    "                suggestions.insert(0, \"**Your y-axis doesn't start at zero.** This can mislead viewers.\")\n",
    "            if 'inconsistent_scale' in violations:\n",
    "                suggestions.insert(0, \"**Different charts use different scales.** Make them uniform for fair comparison.\")\n",
    "    \n",
    "    elif rule == 'Titles':\n",
    "        suggestions.extend([\n",
    "            \"\\n**Use descriptive titles.** Good example: 'Monthly Revenue (EUR) - Q1 2025'. Bad example: 'Chart 1'.\",\n",
    "            \"\\n**Include the 5 W's:** What (Revenue), Where (Netherlands), When (January 2025), how much (in thousands).\",\n",
    "            \"\\n**Put titles at the top** of each chart, not inside it.\",\n",
    "            \"\\n**Keep it concise but clear.** Aim for one line if possible.\"\n",
    "        ])\n",
    "        \n",
    "        if details and 'missing_title' in details.get('violations', []):\n",
    "            suggestions.insert(0, \"**Missing title detected.** Every chart needs a clear heading.\")\n",
    "    \n",
    "    elif rule == 'Color_misuse':\n",
    "        suggestions.extend([\n",
    "            \"**Use color sparingly.** Only highlight what matters—usually negatives (red) or key data points.\",\n",
    "            \"**Stick to IBCS colors:** Blue/grey for normal data, red for negative, green for positive variance.\",\n",
    "            \"**Avoid rainbow charts.** Too many colors make it hard to focus.\",\n",
    "            \"**Test in grayscale.** If your chart doesn't make sense in black and white, you're relying too much on color.\"\n",
    "        ])\n",
    "        \n",
    "        if details and 'excessive_colors' in details.get('violations', []):\n",
    "            suggestions.insert(0, \"**Too many colors detected.** Simplify your color palette.\")\n",
    "    \n",
    "    elif rule == 'Axis_check':\n",
    "        suggestions.extend([\n",
    "            \"**Label both axes clearly.** Include units like '(thousands)', '(%)' or '(days)'.\",\n",
    "            \"**Use readable tick marks.** Not too many (cluttered) or too few (unclear).\",\n",
    "            \"**Rotate labels if needed.** Long category names work better at 45° or vertically.\",\n",
    "            \"**Remove unnecessary gridlines.** Keep only horizontal lines for bar charts, only vertical for column charts.\"\n",
    "        ])\n",
    "        \n",
    "        if details and 'missing_units' in details.get('violations', []):\n",
    "            suggestions.insert(0, \"**Missing units on axis.** Add '(EUR)', '(%)' etc. to your labels.\")\n",
    "    \n",
    "    elif rule == 'Clutter_detection':\n",
    "        suggestions.extend([\n",
    "            \"**Remove decorative elements.** 3D effects, shadows, and borders don't add value.\",\n",
    "            \"**Delete redundant legends.** If you only have one data series, label it directly on the chart.\",\n",
    "            \"**Simplify backgrounds.** Use white or light grey—no patterns or gradients.\",\n",
    "            \"**Cut unnecessary labels.** If every bar is labeled, you don't need y-axis tick marks.\"\n",
    "        ])\n",
    "        \n",
    "        if details and 'excessive_elements' in details.get('violations', []):\n",
    "            suggestions.insert(0, \"**Too many visual elements.** Simplify for better readability.\")\n",
    "    \n",
    "    else:\n",
    "        suggestions.append(f\"Unknown rule: '{rule}'. Please check your configuration.\")\n",
    "    \n",
    "\n",
    "    feedback[\"feedback\"] = suggestions\n",
    "    return feedback\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56b23b",
   "metadata": {},
   "source": [
    "## Feedbacks Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5d0e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_dashboard(\n",
    "    model: nn.Module,\n",
    "    image_path: str,\n",
    "    details_by_rule: Optional[Dict[str, Dict[str, Any]]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    High-level function:\n",
    "    - loads & preprocesses image\n",
    "    - gets model prediction\n",
    "    - calls rule-based feedback\n",
    "    \"\"\"\n",
    "    img_tensor = load_and_preprocess_image(image_path)\n",
    "    pred = predict_dashboard(model, img_tensor)\n",
    "\n",
    "    label = pred[\"label\"]\n",
    "    rule = pred[\"rule\"]\n",
    "    confidence = pred[\"confidence\"]\n",
    "\n",
    "    # For compliant: rule may be None -> use generic \"IBCS\"\n",
    "    if label == \"compliant\":\n",
    "        feedback = generate_feedback(\n",
    "            rule=rule or \"IBCS\",\n",
    "            label=label,\n",
    "            confidence=confidence,\n",
    "            details=None\n",
    "        )\n",
    "    else:\n",
    "        rule_details = None\n",
    "        if details_by_rule is not None and rule in details_by_rule:\n",
    "            rule_details = details_by_rule[rule]\n",
    "\n",
    "        feedback = generate_feedback(\n",
    "            rule=rule,\n",
    "            label=label,\n",
    "            confidence=confidence,\n",
    "            details=rule_details\n",
    "        )\n",
    "\n",
    "    result = {\n",
    "        \"prediction\": pred,\n",
    "        \"feedback\": feedback\n",
    "    }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f58be93",
   "metadata": {},
   "source": [
    "## Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f7470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:176] . file in archive is not in a subdirectory: metadata.json",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Load model (optionally with your fine-tuned checkpoint)\u001b[39;00m\n\u001b[32m      3\u001b[39m     checkpoint = \u001b[33m\"\u001b[39m\u001b[33m../Checkpoints/mobilenet.keras\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     model = \u001b[43mload_mobilenet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Example: fake 'details_by_rule'\u001b[39;00m\n\u001b[32m      7\u001b[39m     fake_details = {\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mscaling\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mviolations\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mnon_zero_start\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33maxis_misaligned\u001b[39m\u001b[33m\"\u001b[39m]},\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtitles\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mviolations\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mmissing_title\u001b[39m\u001b[33m\"\u001b[39m]},\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mclutter_detection\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mviolations\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mexcessive_elements\u001b[39m\u001b[33m\"\u001b[39m]},\n\u001b[32m     13\u001b[39m     }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mload_mobilenet_model\u001b[39m\u001b[34m(checkpoint_path)\u001b[39m\n\u001b[32m      9\u001b[39m model = create_mobilenet_rule_model()\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m os.path.isfile(checkpoint_path):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     state = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     model.load_state_dict(state)\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded model weights from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1491\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1489\u001b[39m orig_position = opened_file.tell()\n\u001b[32m   1490\u001b[39m overall_storage = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m   1492\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[32m   1493\u001b[39m         warnings.warn(\n\u001b[32m   1494\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtorch.load\u001b[39m\u001b[33m'\u001b[39m\u001b[33m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1495\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m dispatching to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtorch.jit.load\u001b[39m\u001b[33m'\u001b[39m\u001b[33m (call \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtorch.jit.load\u001b[39m\u001b[33m'\u001b[39m\u001b[33m directly to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1496\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m silence this warning)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1497\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m   1498\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:771\u001b[39m, in \u001b[36m_open_zipfile_reader.__init__\u001b[39m\u001b[34m(self, name_or_buffer)\u001b[39m\n\u001b[32m    770\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer: Union[\u001b[38;5;28mstr\u001b[39m, IO[\u001b[38;5;28mbytes\u001b[39m]]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m771\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at inline_container.cc:176] . file in archive is not in a subdirectory: metadata.json"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load model (optionally with your fine-tuned checkpoint)\n",
    "    checkpoint = \"./Checkpoints/mobilenet.keras\"\n",
    "    model = load_mobilenet_model(checkpoint_path=checkpoint)\n",
    "\n",
    "    # Example: fake 'details_by_rule'\n",
    "    fake_details = {\n",
    "        \"scaling\": {\"violations\": [\"non_zero_start\", \"axis_misaligned\"]},\n",
    "        \"titles\": {\"violations\": [\"missing_title\"]},\n",
    "        \"color_misuse\": {\"violations\": [\"excessive_colors\"]},\n",
    "        \"axis_check\": {\"violations\": [\"missing_units\"]},\n",
    "        \"clutter_detection\": {\"violations\": [\"excessive_elements\"]},\n",
    "    }\n",
    "\n",
    "    # Run on one image\n",
    "    test_image_path = \"../test.webp\"  # change to your image file\n",
    "    result = explain_dashboard(model, test_image_path, details_by_rule=fake_details)\n",
    "\n",
    "    \n",
    "    print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67a0831a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at: ../Checkpoints\\mobilenet_rules.pth\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"../Checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_path = os.path.join(save_dir, \"mobilenet_rules.pth\")\n",
    "torch.save(model.state_dict(), checkpoint_path)\n",
    "print(\"Model checkpoint saved at:\", checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
